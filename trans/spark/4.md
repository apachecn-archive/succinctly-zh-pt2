# 四、使用 Spark 的数据输入和输出

到目前为止，我们已经在 Spark 中浏览了相当多的关于数据处理主题的概念和方法。到目前为止，我们主要将数据输出到磁盘上的文本文件中，或者简单地使用 Scala 和 Python 的交互式外壳。当涉及到学习时，交互式外壳和常规文本文件是非常好的，这种方法没有错。但是迟早你会遇到这样的情况，你必须从一些标准的输入源中读取数据，或者以环境中其他进程能够使用的方式存储数据。Spark 支持大量的输入和输出功能。我们不会一一列举，但会提到最重要的。在本章中，我们还将介绍 Spark 如何将数据存储到像 Cassandra 这样的流行存储技术中。让我们从简单的文本文件开始。

## 处理文本文件

我们已经处理了一些例子，其中我们使用文本文件作为例子的输入和输出。作为一个小复习，让我们看一下如何制作代表文本文件中每一行的 rdd:

代码清单 92:在 Scala 中作为 RDD 加载的文本文件

```scala
      scala> val input = sc.textFile("README.md")

      input: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[31] at textFile at <console>:21

```

代码清单 93:Python 中作为 RDD 加载的文本文件

```scala
      >>> input = sc.textFile("README.md")

```

代码清单 94:在 Java 中作为 RDD 加载的文本文件

```scala
      JavaRDD<String> inputFile = context.textFile("README.md");

```

我们在本书前面的字数统计示例中讨论了如何使用创建的 RDD，因此我们现在不需要详细讨论。然而，有一个非常有用的特性。也就是说，有时数据无法放入一个文件中。例如，如果我们有非常大的日志，其中时不时会创建一个新的日志文件，我们可以使用类似“*”的通配符将它们一次全部读入 RDD。一次全部不是字面上的意思“一次检查整个文件并将其加载到内存中”；请记住，rdd 是延迟加载的，因此在调用操作之前，不会读取数据。为了确保我们在同一个页面上，我们将分析我们创建的一个简单文件。这将是每天售出的二手车数量。我们不会把任何事情复杂化；我们只创建两个文件。将它们放在文件系统中您选择的文件夹中:

代码清单 95:销售单位示例文件

```scala
      1000
      2000
      1000
      5000
      4000
      10000
      11000
      300
      5500

```

代码清单 96:销售单位示例文件

```scala
      4000
      5000
      6000
      6000
      3000
      4000
      500
      700
      8800

```

现在我们将制作示例程序，分析数据并计算两个月内的平均销量。让我们从 Scala 开始:

代码清单 97:读取已售单位的 Scala 示例

```scala
      val salesRaw = sc.wholeTextFiles("sold-units-2015*")

      val sales = salesRaw.map(x => x._2).
          flatMap(x => x.split("\n")).
          map(x => x.toDouble)

```

请注意，RDD 元素存储文件描述和文件内容。我们只对内容部分感兴趣，所以我们通过元组的第二部分进行映射。假设我们对平均值感兴趣。到目前为止，我们还没有提到 spark 支持只包含双精度的 rdd 上的算术函数。在 Spark 中，计算平均值实际上非常容易:

代码清单 98:显示售出单位平均价格的 Scala

```scala
      scala> sales.mean()

      res30: Double = 4322.222222222223

```

让我们用 Python 做同样的事情:

代码清单 99:读取售出单位的 Python 示例

```scala
      sales_raw = sc.wholeTextFiles("sold-units-2015*")

      sales = sales_raw.map(lambda x: x[1]
      ).flatMap(lambda x: x.split("\n")
      ).map(lambda x: float(x))

```

代码清单 100:显示售出单位平均价格的 Python

```scala
      >>> sales.mean()

      4322.222222222223

```

让我们来看看 Java 版本:

代码清单 101:销售单位平均价格的 Java 示例

```scala
      package com.syncfusion.succinctly.spark.examples;

      import java.io.FileNotFoundException;
      import java.io.PrintWriter;
      import java.util.Arrays;

      import org.apache.spark.SparkConf;
      import org.apache.spark.api.java.JavaDoubleRDD;
      import org.apache.spark.api.java.JavaPairRDD;
      import org.apache.spark.api.java.JavaSparkContext;
      import org.apache.spark.api.java.function.DoubleFunction;
      import org.apache.spark.api.java.function.FlatMapFunction;
      import org.apache.spark.api.java.function.Function;
      import scala.Tuple2;

      public class WholeTextFileMeanUnitPriceJava7 {

                      public static void main(String[] args) throws FileNotFoundException {
                                      if (args.length < 1) {
                                                      System.err.println("Please provide a full path to the input files");
                                                      System.exit(0);
                                      }

                                      if (args.length < 2) {
                                                      System.err.println("Please provide a full path to the output file");
                                                      System.exit(0);
                                      }

                                      SparkConf conf = new SparkConf()
                                                                      .setAppName("WholeTextFileJava7")
                                                                      .setMaster("local");
                                      JavaSparkContext context = new JavaSparkContext(conf);

                                      JavaPairRDD<String, String> salesRaw = context.wholeTextFiles(args[0]);

                                      JavaDoubleRDD sales = salesRaw.map(new Function<Tuple2<String, String>, String>() {
                                                      public String call(Tuple2<String, String> v1) throws Exception {
                                                                      return v1._2();
                                                      }
                                      }).flatMap(new FlatMapFunction<String, String>() {
                                                      public Iterable<String> call(String t) throws Exception {
                                                                      return Arrays.asList(t.split("\n"));
                                                      }

                                      }).mapToDouble(new DoubleFunction<String>() {
                                                      @Override
                                                      public double call(String t) throws Exception {
                                                                      return Double.parseDouble(t);
                                                      }
                                      });

                                      PrintWriter writer = new PrintWriter(args[1]);
                                      writer.println(sales.mean());
                                      writer.close();
                      }
      }

```

向 Spark 提交任务时要小心。我们希望所有具有模式的文件都由 Spark 解析，而不是由 shell 解析。下面是我如何向 Spark 提交 Java 任务的:

代码清单 102–Linux 上的任务提交操作

```scala
      $ ./bin/spark-submit \
      --class com.syncfusion.succinctly.spark.examples. WholeTextFileMeanUnitPriceJava7 \
      --master local \
      /root/spark/SparkExamples-0.0.1-SNAPSHOT.jar \
      "/root/spark-1.4.1-bin-hadoop2.6/sold-units*" /root/spark/result.txt

      $ cat /root/spark/result.txt

      4322.222222222223

```

如果您提交的任务没有报价，您可能会遇到麻烦，因为输出将转到其中一个输入文件，并且您在运行时不会得到任何反馈。您还将覆盖输入文件。

## 向 Spark 提交 Scala 和 Python 任务

到目前为止，我们还不需要直接向 Spark 提交 Python 和 Scala 任务。我们只是简单地使用了交互式外壳，一切都很好。我们只展示了如何向 Spark 提交 Java 应用程序，因为 Java 目前没有交互式外壳。在我看来，最好在你需要的时候获取信息，因为否则你可能只是浏览了本书前面部分的内容，然后就完全忘记了。现在你会需要它。在本章中，我们将在 Spark 的上下文中讨论 JSON 序列化，因此我们需要在项目中包含一些库。我们将再次制作一个简单的字数统计应用程序。首先，您必须创建一个文件系统目录和文件结构，如下所示:

代码清单 103:基本的 Scala 项目结构

```scala
      $ tree
      .
      ├── simple.sbt
      └── src
          └── main
              └── scala
                  └── ExampleApp.scala

```

代码清单 104:简单. sbt 文件的文件内容

```scala
      name := "Example application"

      version := "1.0"

      scalaVersion := "2.10.5"

      libraryDependencies += "org.apache.spark" %% "spark-core" % "1.4.1"

```

代码清单 105:示例文件的文件内容

```scala
      import org.apache.spark.SparkContext
      import org.apache.spark.SparkContext._
      import org.apache.spark.SparkConf

      object ExampleApp {
        def main(args: Array[String]) {
          val conf = new SparkConf().setAppName("ExampleApp")
          val sc = new SparkContext(conf)

          val readmeFile = sc.textFile(args(0))
          val sparkMentions = readmeFile.filter(line => line.contains("Spark"))

          scala.tools.nsc.io.File(args(1)).writeAll(sparkMentions.count().toString)
        }
      }

```

创建目录结构和编辑文件后，运行 package 命令。这将创建一个`.jar`文件。这个过程非常类似于构建一个 Java 应用程序:

代码清单 106:运行包命令来构建。文件 jar

```scala
      $ sbt package

```

这将导致创建`./target/scala-2.10/example-application_2.10-1.0.jar`文件。如果您更改版本，名称可能会有所不同。向 Spark 提交任务与提交 Java 任务非常相似:

代码清单 107:向 Spark 提交 Scala 示例

```scala
      $ ./bin/spark-submit \
      --class ExampleApp \
      --master local \
      /root/spark/scala/ExampleApp/target/scala-2.10/example-application_2.10-1.0.jar \
      /root/spark-1.4.1-bin-hadoop2.6/README.md /root/spark/scala_example.txt

```

使用 Python 向 Spark 提交任务相对简单。第一部分是在文件系统的某个地方创建 Python 脚本。让我们称之为类似`simple.py`的简单事物:

代码清单 108: Python 脚本简单

```scala
      import sys

      from PySpark import SparkContext

      sc = SparkContext(appName="ExampleApp")

      readme_file = sc.textFile(sys.argv[1])
      spark_mentions = readme_file.filter(lambda line: "Spark" in line)

      out_file = open(sys.argv[2], 'w+')
      out_file.write(str(spark_mentions.count()))

```

针对 Spark 运行该示例相对容易:

代码清单 109: Python 脚本简单

```scala
      $ ./bin/spark-submit \
      --master local \
      /root/spark/python/simple.py \
      /root/spark-1.4.1-bin-hadoop2.6/README.md /root/spark/python_example.txt

```

在下一节中，我们将讨论使用 JSON 的技术。

## 使用 JSON 文件

JSON 是当今最流行的数据格式之一。它无处不在，从配置文件到网页通信。这就是我们将介绍如何使用 Spark 的主要原因之一。JSON 加载最简单的方法之一是将外部数据视为文本，然后使用库对其进行映射。写出来也一样。我们将创建一个简单的 JSON 文件用于我们的示例:

代码清单 110:带有多个 JSON 条目的简单文件

```scala
      {"year": 2012, "name": "Honda Accord LX", "color": "gray"}
      {"year": 2008, "name": "Audi Q7 4.2 Premium", "color": "white"}
      {"year": 2006, "name": "Mercedes-Benz CLS-Class CLS500", "color": "black"}
      {"year": 2012, "name": "Mitsubishi Lancer", "color": "black"}
      {"year": 2010, "name": "BMW 3 Series 328i", "color": "gray"}
      {"year": 2006, "name": "Buick Lucerne CX", "color": "gray"}
      {"year": 2006, "name": "GMC Envoy", "color": "red"}

```

让我们用 Scala 加载它，然后按年份对数据进行排序:

代码清单 111:加载和排序测试的 Scala 代码

```scala
      import org.apache.spark.SparkContext
      import org.apache.spark.SparkContext._
      import org.apache.spark.SparkConf

      import com.fasterxml.jackson.module.scala._
      import com.fasterxml.jackson.databind.ObjectMapper
      import com.fasterxml.jackson.databind.DeserializationFeature

      import java.io.StringWriter;

      object JsonLoading {
        case class UsedCar(year: Int, name: String, color: String) extends Serializable

        def main(args: Array[String]) {
              val conf = new SparkConf().setAppName("JsonLoading")
              val sc = new SparkContext(conf)

              // mapper can't get serialized by spark at the moment
              // using workaround
              object Holder extends Serializable {
                @transient lazy val mapper = new ObjectMapper()
                mapper.registerModule(DefaultScalaModule)
              }

              val inputFile = sc.textFile(args(0))
              val usedCars = inputFile.map(x => Holder.mapper.readValue(x, classOf[UsedCar]))

              val carsByYear = usedCars.map(x => (x.year, x))

              val carsSortedByYear = carsByYear.sortByKey(false).collect()

              val out = new StringWriter()
              Holder.mapper.writeValue(out, carsSortedByYear)
              val json = out.toString()

              scala.tools.nsc.io.File(args(1)).writeAll(json)
          }
      }

```

生成的文件如下所示:

代码清单 112:生成的 JSON 文件

```scala
      [
        [
          2012,
          {
            "year": 2012,
            "name": "Honda Accord LX",
            "color": "gray"
          }
        ],
        [
          2012,
          {
            "year": 2012,
            "name": "Mitsubishi Lancer",
            "color": "black"
          }
        ],
        [
          2010,
          {
            "year": 2010,
            "name": "BMW 3 Series 328i",
            "color": "gray"
          }
        ],
        [
          2008,
          {
            "year": 2008,
            "name": "Audi Q7 4.2 Premium",
            "color": "white"
          }
        ],
        [
          2006,
          {
            "year": 2006,
            "name": "Mercedes-Benz CLS-Class CLS500",
            "color": "black"
          }
        ],
        [
          2006,
          {
            "year": 2006,
            "name": "Buick Lucerne CX",
            "color": "gray"
          }
        ],
        [
          2006,
          {
            "year": 2006,
            "name": "GMC Envoy",
            "color": "red"
          }
        ]
      ]

```

现在让我们用 Python 做一些类似的事情:

代码清单 113:对 JSON 中的条目进行排序的 Python 脚本

```scala
      import sys
      import json

      from PySpark import SparkContext

      sc = SparkContext(appName="JsonLoading")

      input_file = sc.textFile(sys.argv[1])
      used_cars = input_file.map(lambda x: json.loads(x))

      cars_by_year = used_cars.map(lambda x: (x["year"], x))

      cars_sorted_by_year = cars_by_year.sortByKey(False)

      mapped = cars_sorted_by_year.map(lambda x: json.dumps(x))

      mapped.saveAsTextFile(sys.argv[2])

```

提交 Python 脚本后，您应该会得到以下输出:

代码清单 114:提交 Python 脚本的结果

```scala
      [2012, {"color": "gray", "name": "Honda Accord LX", "year": 2012}]
      [2012, {"color": "black", "name": "Mitsubishi Lancer", "year": 2012}]
      [2010, {"color": "gray", "name": "BMW 3 Series 328i", "year": 2010}]
      [2008, {"color": "white", "name": "Audi Q7 4.2 Premium", "year": 2008}]
      [2006, {"color": "black", "name": "Mercedes-Benz CLS-Class CLS500", "year": 2006}]
      [2006, {"color": "gray", "name": "Buick Lucerne CX", "year": 2006}]
      [2006, {"color": "red", "name": "GMC Envoy", "year": 2006}]

```

作为本节的最后一个例子，让我们看看如何用 Java 进行 JSON 操作:

代码清单 115: Java 操纵 JSON

```scala
      package com.syncfusion.succinctly.spark.examples;

      import java.io.FileNotFoundException;
      import java.io.PrintWriter;
      import java.io.Serializable;
      import java.util.List;

      import org.apache.spark.SparkConf;
      import org.apache.spark.api.java.JavaPairRDD;
      import org.apache.spark.api.java.JavaRDD;
      import org.apache.spark.api.java.JavaSparkContext;
      import org.apache.spark.api.java.function.Function;
      import org.apache.spark.api.java.function.PairFunction;

      import com.fasterxml.jackson.core.JsonProcessingException;
      import com.fasterxml.jackson.databind.ObjectMapper;

      import scala.Tuple2;

      public class JsonManipulationJava7 {

                      public static class UsedCar implements Serializable {
                                      public int year;
                                      public String name;
                                      public String color;

                                      // for object mapping
                                      public UsedCar() {

                                      }

                                      public UsedCar(int year, String name, String color) {
                                                      this.year = year;
                                                      this.name = name;
                                                      this.color = color;
                                      }
                      }

                      public static void main(String[] args) throws FileNotFoundException, JsonProcessingException {
                                      if (args.length < 1) {
                                                      System.err.println("Please provide a full path to the input files");
                                                      System.exit(0);
                                      }

                                      if (args.length < 2) {
                                                      System.err.println("Please provide a full path to the output file");
                                                      System.exit(0);
                                      }

                                      SparkConf conf = new SparkConf()
                                                                      .setAppName("JsonManipulationJava7")
                                                                      .setMaster("local");
                                      JavaSparkContext sc = new JavaSparkContext(conf);

                                      JavaRDD<String> inputFile = sc.textFile(args[0]);

                                      ObjectMapper mapper = new ObjectMapper();

                                      JavaRDD <UsedCar> usedCars = inputFile.map(new Function<String, UsedCar>() {
                                                      public UsedCar call(String v1) throws Exception {
                                                                      return mapper.readValue(v1, UsedCar.class);
                                                      }
                                      });

                                      JavaPairRDD<Integer, UsedCar> carsByYear = usedCars.mapToPair(new PairFunction<UsedCar, Integer, UsedCar>() {
                                                      @Override
                                                      public Tuple2<Integer, UsedCar> call(UsedCar t) throws Exception {
                                                                      return new Tuple2<Integer, UsedCar>(t.year, t);
                                                      }
                                      });

                                      List<Tuple2<Integer, UsedCar>> carsSortedByYear = carsByYear.sortByKey(false).collect();

                                      PrintWriter writer = new PrintWriter(args[1]);
                                      writer.println(mapper.writeValueAsString(carsSortedByYear));
                                      writer.close();
                      }
      }

```

Java 还演示了将元组导出为 JSON 格式。只是为了让你能看到它是什么样子:

代码清单 116:Java JSON 操作的结果

```scala
      [
        {
          "_1": 2012,
          "_2": {
            "year": 2012,
            "name": "Honda Accord LX",
            "color": "gray"
          }
        },
        {
          "_1": 2012,
          "_2": {
            "year": 2012,
            "name": "Mitsubishi Lancer",
            "color": "black"
          }
        },
        {
          "_1": 2010,
          "_2": {
            "year": 2010,
            "name": "BMW 3 Series 328i",
            "color": "gray"
          }
        },
        {
          "_1": 2008,
          "_2": {
            "year": 2008,
            "name": "Audi Q7 4.2 Premium",
            "color": "white"
          }
        },
        {
          "_1": 2006,
          "_2": {
            "year": 2006,
            "name": "Mercedes-Benz CLS-Class CLS500",
            "color": "black"
          }
        },
        {
          "_1": 2006,
          "_2": {
            "year": 2006,
            "name": "Buick Lucerne CX",
            "color": "gray"
          }
        },
        {
          "_1": 2006,
          "_2": {
            "year": 2006,
            "name": "GMC Envoy",
            "color": "red"
          }
        }
      ]

```

还有很多其他支持的格式，包括 CSV、序列文件等。我们不会深入讨论它们，因为我们刚刚讨论了最基本的，但是还有一件非常重要的事情我们还没有讨论。这就是 Spark 与数据库交互的方式。Spark 非常支持与关系数据库交互。Spark 也经常被描述为一个大数据处理框架。因此，也许最好专注于当今最流行的大数据存储技术之一。是 Apache Cassandra，和 Spark 配合非常好。

## Spark 和卡珊德拉

Spark 和 Cassandra 是天生的一对:一边是大数据处理框架，另一边是大数据存储技术。现在，老实说，我可能有点偏见，因为我经常和卡珊德拉一起工作，我个人的观点是，最好谈谈你知道的事情。我还想用 Spark 和 Cassandra 做一个完整的、端到端的例子。

我们最好从安装卡珊德拉开始。在前几章中，我们描述了如何在您的计算机上安装 Java。这足以让卡珊德拉有所作为。你的下一步是下载卡珊德拉本身。有一个商业版本可以从一家名为 DataStax 的公司获得，但是让你开始使用社区版本就可以了。前往[http://www.planetcassandra.org/cassandra/](http://www.planetcassandra.org/cassandra/)下载适合您系统的版本。我不会说太多细节，因为这真的超出了本书的范围，但是你可以在[http://www.planetcassandra.org/try-cassandra/](http://www.planetcassandra.org/try-cassandra/)下找到你需要的一切。有很多关于如何设置 Cassandra 的教程，但是正如我前面提到的，最重要的先决条件是已安装的 Java 版本。如果你喜欢使用 Syncfusion 的资源，在[http://www.syncfusion.com/resources/techportal/ebooks](http://www.syncfusion.com/resources/techportal/ebooks)有很多免费的书籍，而*卡珊德拉简洁地*就是其中之一。所以不用多说，从现在开始，我假设您的本地机器上有一个正在运行的 Cassandra 实例。这一部分可能比前面的部分更具挑战性，但我会尽量提供更多细节。

卡珊德拉的一个优点是，它可以在相对较短的时间内存储大量数据。这都要归功于卡珊德拉的内在，其核心是宽行的概念。我们不会从理论的角度深入讨论，但我认为你可以通过例子来理解。Cassandra 目前不支持做 sum、average 等聚合运算。唯一可用的操作是计数。聚合操作是巨大的性能杀手，即使在关系世界中也是如此。迟早会有一个查询花费太长时间，然后管理员或工程师进入其中，他们最常做的是创建某种类型的表或视图，这些表或视图立即准备好了数据，因此不需要连接。这实际上是你如何用卡珊德拉建模数据，但说实话，这是一个自己的话题。我们将稍微谈谈内部。与传统数据库不同，Cassandra 每行最多可以有 20 亿列。这就是宽行这个名字的由来。卡珊德拉一直保持所有这些列的排序。您可以通过排序的列来访问这些值，但是您不能像使用关系数据库那样计算平均值和类似的东西，所以您必须使用其他技术，例如我们的 Spark。

导航到安装了 Cassandra 的文件夹，运行名为`cqlsh`的工具。它代表卡珊德拉查询语言外壳。我们将使用这个 shell 设置数据，然后使用 Spark 处理数据，并将分析结果存储回 Spark。这主要是介绍性的材料，所以我会尽量保持简单。我不会总是使用最佳生产水平实践，因为我的目标是让你开始。如果有这方面经验的人正在读这篇文章，请不要对我有偏见。开始的`cqlsh`应该是这样的:

代码清单 117:运行 CQL Shell

```scala
      $ ./cqlsh
      Connected to Test Cluster at 127.0.0.1:9042.
      [cqlsh 5.0.1 | Cassandra 2.1.8 | CQL spec 3.2.0 | Native protocol v3]
      Use HELP for help.
      cqlsh>

```

如果您是第一次运行集群，您没有定义用于存储数据的键空间和表。不要过多考虑键空间；现在，你可以把它看作是一个桌子的容器。我们来定义一个。事实上，让我们马上定义其中的两个。Spark 将从一个容器中读取数据，并将其写入另一个容器。生产中通常有多个键槽。但是，请注意，这不是生产级别的示例:

代码清单 118:卡珊德拉气象站示例的键空间定义

```scala
      CREATE KEYSPACE weather_in
      WITH REPLICATION = {
          'class' : 'SimpleStrategy',
          'replication_factor' : 1
      };

      CREATE KEYSPACE weather_out
      WITH REPLICATION = {
          'class' : 'SimpleStrategy',
          'replication_factor' : 1
      };

```

让我们为输入数据创建一个表:

代码清单 119:卡珊德拉气象站示例的表格定义

```scala
      use weather_in;

      CREATE TABLE measurements (
          station text,
          time timestamp,
          temperature decimal,
          PRIMARY KEY (station, time)
      );

```

主键语法的工作原理与关系数据库有点不同。这里，主键的第一部分指定哪些数据将有一个长行。在我们的例子中，每个气象站都有一大排。主键语法的第二部分指定如何在这一长行中对数据进行排序。在我们的情况下，它将按时间排序。本质上，每次一个读数出现，它都会到达一个长行的末尾。

让我们自己定义一些数据。我们不在乎这个数据是摄氏还是华氏，所以我们将把数值保持在零左右，这样在两个尺度上都是可能的。差别会越来越小，这取决于您喜欢哪个单元:

代码清单 120:一些数据(想象它来自气象站)

```scala
      INSERT INTO measurements (station, time, temperature) VALUES ('A', '2015-12-01 10:00:00', 1);
      INSERT INTO measurements (station, time, temperature) VALUES ('A', '2015-12-01 12:00:00', 2);
      INSERT INTO measurements (station, time, temperature) VALUES ('A', '2015-12-02 10:00:00', 3);
      INSERT INTO measurements (station, time, temperature) VALUES ('A', '2015-12-02 12:00:00', 5);
      INSERT INTO measurements (station, time, temperature) VALUES ('A', '2015-12-02 14:00:00', 5);

      INSERT INTO measurements (station, time, temperature) VALUES ('B', '2015-12-01 10:00:00', 3);
      INSERT INTO measurements (station, time, temperature) VALUES ('B', '2015-12-01 12:00:00', 4);
      INSERT INTO measurements (station, time, temperature) VALUES ('B', '2015-12-02 10:00:00', 0);
      INSERT INTO measurements (station, time, temperature) VALUES ('B', '2015-12-02 12:00:00', 2);
      INSERT INTO measurements (station, time, temperature) VALUES ('B', '2015-12-02 14:00:00', 1);

```

您可以使用星号运算符选择所有数据。但这不是生产中的最佳实践；卡珊德拉甚至有一个默认的 10，000 条记录的限制，这样你就不会在生产中打破任何东西。获取数据的正确方法是提供一个测量站。为了简单起见，我们只有站点 A 和站点 B。如果我们想知道站点 A 和站点 B 在某个时间附近的温度，我们将毫无问题地获得数据，因为测量是按时间排序的。但是，如果我们试图选择温度高于 2 度以下的所有测量值，就会出现这种情况:

代码清单 121:尝试选择高于某个值的所有温度

```scala
      cqlsh:weather_in> SELECT * from measurements where station = 'A' AND temperature > 2;

      InvalidRequest: code=2200 [Invalid query] message="No secondary indexes on the restricted columns support the provided operators: "

```

这就是为什么我们需要 Spark。假设我们对气象站的历史平均值感兴趣。我们将保持简单，所以我们只需要存储一个气象站，平均值和计算平均值的准确时间。我们将让 Spark 填充输出表。要连接到卡珊德拉，我们将使用 Spark 卡珊德拉连接器，可在[https://github.com/datastax/spark-cassandra-connector](https://github.com/datastax/spark-cassandra-connector)下获得。在本节中，我们将只讨论 Scala 代码。您可以在 Spark Cassandra Connector 上查找教程，并使用您喜欢的语言执行这里提供的示例。我们在这里介绍的概念足以让您入门。

前几章我们已经讨论了如何制作独立的 Spark 应用程序。为了连接到 Cassandra，我们需要添加新的依赖项。问题是依赖项还必须包含一些其他的依赖项。如果我们想向 Spark 提交任务，Spark 将无法定位引用。jar 文件。我们将不得不稍微调整一下项目结构，以便能够构建一个所谓的胖罐子。

如果我们想连接到 Cassandra，我们需要添加一个新的依赖项。另外，当我们构建这个胖罐子时，它不应该包含已经包含在 Spark 中的文件。为了实现这个目标，我们将使用一个非常受欢迎的工具，该工具在 https://github.com/sbt/sbt-assembly/上市。我们将不得不稍微修改一下 Spark 示例的结构。我们必须在示例的根目录中创建新文件`build.sbt`。该文件应包含以下内容:

代码清单 122:在 Scala 示例目录中创建新的 build.sbt 文件

```scala
      lazy val commonSettings = Seq(
        version := "0.1-SNAPSHOT",
        organization := "syncfusion",
        scalaVersion := "2.10.5"
      )

      lazy val app = (project in file("app")).
        settings(commonSettings: _*).
        settings(
        )

```

更新`simple.sbt`文件内容:

代码清单 123:在 Scala 示例目录中创建新的 simple.sbt 文件

```scala
      name := "example-application"

      version := "1.0"

      scalaVersion := "2.10.5"

      libraryDependencies += "org.apache.spark" %% "spark-core" % "1.4.1" % "provided"
      libraryDependencies += "com.datastax.spark" %% "spark-cassandra-connector" % "1.4.0-M3"
      libraryDependencies += "com.datastax.spark" %% "spark-cassandra-connector-java" % "1.4.0-M3"

```

创建新的 Scala 文件:

代码清单 124:源代码文件

```scala
      import org.apache.spark.SparkContext
      import org.apache.spark.SparkContext._
      import org.apache.spark.SparkConf

      import com.datastax.spark.connector._
      import com.datastax.driver.core._

      import java.util.Date

      object CassandraProcessing {
        case class Average(station: String, time: Date, average: BigDecimal)

        def main(args: Array[String]) {
          val conf = new SparkConf()
          .setAppName("CassandraProcessing")
          .set("spark.cassandra.connection.host", "127.0.0.1")

          val sc = new SparkContext(conf)

          val measurements = sc.cassandraTable("weather_in", "measurements")

          val measurementPairs = measurements.map(row => (row.getString("station"), row.getDecimal("temperature")))

          val averages = measurementPairs.mapValues(x => (x, 1)).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2))

          var averagesForCassandra = averages.map(x => new Average(x._1, new Date(), x._2._1 / x._2._2))

          averagesForCassandra.saveToCassandra("weather_out", "all_time_average")
        }
      }

```

在项目文件夹中创建`assembly.sbt`文件，内容如下:

代码清单 125:文件项目/程序集

```scala
      addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.13.0")

```

现在通过运行 assembly 命令创建一个胖罐子:

代码清单 126:运行汇编命令

```scala
      $ sbt assembly

```

观察汇编脚本的输出，并记住生成的 jar 的位置。输出将不同于前面的例子。以下是我的提交任务的外观:

代码清单 127:提交任务

```scala
      $ ./bin/spark-submit \
      --class CassandraProcessing \
      --master local \
      /root/spark/scala/ExampleApp/target/scala-2.10/example-application-assembly-1.0.jar

```

如果一切顺利，你应该可以在卡珊德拉看到处理结果。

代码清单 128:卡珊德拉的处理结果

```scala
      cqlsh> use weather_out;
      cqlsh> select * from all_time_average;

       station | time                                 | average
      -----------+-------------------------------------+---------
             B     | 2015-08-17 22:12:25+0200 |       2
             A     | 2015-08-17 22:12:25+0200 |     3.2

      (2 rows)

```

在之前的代码清单中，该表包含了历史平均值的计算结果和计算该平均值时的时间戳。这个例子包含了所有的基本技术，如果你有机会在实践中结合两种技术，你将会用到这些技术。提供的示例充满了可应用于各种任务的模式。

请注意，尽管 Spark 和 Cassandra 是相对容易学习的技术，但在生产方面，它们有许多您必须了解的最佳实践。从某种意义上说，我们只是触及了表面。但是，即使是最长的旅程也是从第一步开始的，我希望这一部分会给你的生活带来一条全新的道路。

在这一节中，我们讲述了你掌握 Spark 之路上的一个非常重要的里程碑。请注意，Spark 有一个非常活跃的社区，您可以通过查阅在线资料和参观 Spark 会议和集会来进一步发展自己。我希望你喜欢这本电子书和提供的例子。让我们继续得出结论。