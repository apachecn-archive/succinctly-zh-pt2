# 第 3 章线程、任务和异步/等待

为了开始研究我们的体系结构，我们真的需要深入研究线程问题。一路上，我们会发现一些令人惊讶的事情。

对于如何处理传入的请求，有两个基本选项:

*   多个侦听器:我们创建多个侦听器，并在分配给等待的`GetContextAsync`调用的继续的线程上处理请求。因为没有 Windows 窗体，所以延续可以自由分配自己的线程，这与 Windows 应用程序行为相反，后者封送到主应用程序线程上。
*   单个侦听器:单个线程侦听传入的连接，并立即将该请求排队，以便它可以返回侦听下一个连接请求。一个单独的线程(或多个线程)处理请求。

本节介绍的源代码位于[比特桶存储库](https://bitbucket.org/syncfusiontech/web-servers-succinctly)的文件夹**示例\第 3 章\演示-异步等待**中。

## 多个听众

让我们看看在[之前的代码](02.html#StartConnectionListener)中检测`StartConnectionListener`函数，这样我们就可以了解处理时间和线程。首先，我们将在`Program`类中添加几个基本的检测函数:

```cs
    protected static DateTime timestampStart;

    static public void TimeStampStart()
    {
      timestampStart = DateTime.Now;
    }

    static public void TimeStamp(string msg)
    {
      long elapsed = (long)(DateTime.Now - timestampStart).TotalMilliseconds;
      Console.WriteLine("{0} : {1}", elapsed, msg);
    }

```

代码清单 6

接下来，我们将工具添加到`StartConnectionListener`中，用关于侦听器何时以及在哪个线程上启动的信息来替换之前的方法。我还用一个通用的“处理程序”对象代替了对响应的处理(下面将描述)。

```cs
    /// <summary>
    /// Await connections.
    /// </summary>
    static async void StartConnectionListener(HttpListener listener)
    {
      TimeStamp("StartConnectionListener Thread ID: " + Thread.CurrentThread.ManagedThreadId);

      // Wait for a connection. Return to caller while we wait.
      HttpListenerContext context = await listener.GetContextAsync();

      // Release the semaphore so that another listener can be immediately started up.
      sem.Release();

      handler.Process(context);
    }

```

代码清单 7

回想一下，这些侦听器都是在一个单独的线程上初始化的，但是如前所述，我们让。NET 框架在延续上分配一个线程。这里还是第 2 章中初始化监听器的代码:

```cs
    Task.Run(() =>
    {
      while (true)
      {
        sem.WaitOne();
        StartConnectionListener(listener);
      }
    });

```

代码清单 8

为了这个测试，我创建了一个`ListenerThreadHandler`类:

```cs
    public class ListenerThreadHandler : CommonHandler, IRequestHandler
    {
      public void Process(HttpListenerContext context)
      {
        Program.TimeStamp("Process Thread ID: " + Thread.CurrentThread.ManagedThreadId);
        CommonResponse(context);
      }
    }

```

代码清单 9

`CommonResponse`(一种`ListenerThreadHandler`的方法)在发出响应之前，人为地注入一秒钟的延迟来模拟一些复杂的过程:

```cs
    public void CommonResponse(HttpListenerContext context)
    {
      // Artificial delay.
      Thread.Sleep(1000);

      // Get the request.
      HttpListenerRequest request = context.Request;
      HttpListenerResponse response = context.Response;

      // Get the path, everything up to the first ? and excluding the leading "/"
      string path = request.RawUrl.LeftOf("?").RightOf("/");

      // Load the file and respond with a UTF8 encoded version of it.
      string text = File.ReadAllText(path);
      byte[] data = Encoding.UTF8.GetBytes(text);
      response.ContentType = "text/html";
      response.ContentLength64 = data.Length;
      response.OutputStream.Write(data, 0, data.Length);
      response.ContentEncoding = Encoding.UTF8;
      response.StatusCode = 200; // OK
      response.OutputStream.Close();
    }

```

代码清单 10

处理程序对象在`Main`中实例化:

```cs
    static void Main(string[] args)
    {
      // Supports 20 simultaneous connections.
      sem = new Semaphore(20, 20);
      handler = new ListenerThreadHandler();
    ...etc...

```

代码清单 11

初始化侦听器后，我们将向`Main`添加一个测试，以查看服务器如何有效地响应 10 个同时的异步请求:

```cs
    TimeStampStart();

    for (int i = 0; i < 10; i++)
    {
      Console.WriteLine("Request #" + i);
      MakeRequest(i);
    }

```

代码清单 12

以及:

```cs
    /// <summary>
    /// Issue GET request to localhost/index.html
    /// </summary>
    static async void MakeRequest(int i)
    {
      TimeStamp("MakeRequest " + i + " start, Thread ID: " + Thread.CurrentThread.ManagedThreadId);
      string ret = await RequestIssuer.HttpGet("http://localhost/index.html");
      TimeStamp("MakeRequest " + i + " end, Thread ID: " + Thread.CurrentThread.ManagedThreadId);
    }

```

代码清单 13

`RequestIssuer`是一个“可唤醒”的请求和响应功能，意味着它将发出 web 请求，并在等待响应的同时返回给调用者。响应在`await`继续中处理:

```cs
    public class RequestIssuer
    {
      public static async Task<string> HttpGet(string url)
      {
        string ret;

        try
        {
          HttpWebRequest request = (HttpWebRequest)HttpWebRequest.Create(url);
          request.Method = "GET";

          using (WebResponse response = await request.GetResponseAsync())
          {
            using (StreamReader reader = new StreamReader(response.GetResponseStream()))
            {
              ret = await reader.ReadToEndAsync();
            }
          }
        }
        catch (Exception ex)
        {
          ret = ex.Message;
        }

        return ret;
      }
    }

```

代码清单 14

在前面的代码中，一旦一个异步函数阻塞，`await`将返回给调用者，并发出下一个`MakeRequest`。当异步功能完成时，`MakeRequest`继续。

### 测试结果

我们想知道的是:

*   请求是什么时候发出的？
*   完成需要多长时间？
*   延续与请求调用在同一个线程上，还是在不同的线程上？

在跟踪日志中，我们首先看到所有的`MakeRequest`函数调用都在同一个线程上，这是意料之中的，因为它们都是由同一个`Task`发出的:

```cs
    Request #0
     3 : MakeRequest 0 start, Thread ID: 1
     Request #1
     55 : MakeRequest 1 start, Thread ID: 1
     Request #2
     57 : MakeRequest 2 start, Thread ID: 1
     Request #3
     58 : MakeRequest 3 start, Thread ID: 1
     Request #4
     59 : MakeRequest 4 start, Thread ID: 1
     Request #5
     61 : MakeRequest 5 start, Thread ID: 1
     Request #6
     62 : MakeRequest 6 start, Thread ID: 1
     Request #7
     63 : MakeRequest 7 start, Thread ID: 1
     Request #8
     63 : MakeRequest 8 start, Thread ID: 1
     Request #9
     63 : MakeRequest 9 start, Thread ID: 1

```

代码清单 15

接下来，我们看到流程消息以及`MakeRequest`“结束”呼叫(为了清楚起见，我省略了`StartConnectionListener`和`MakeRequest`消息):

```cs
    78 : Process Thread ID: 11
    79 : Process Thread ID: 5
    80 : Process Thread ID: 9
    81 : Process Thread ID: 10

    783 : Process Thread ID: 12

    1080 : Process Thread ID: 11
    1084 : Process Thread ID: 5
    1091 : Process Thread ID: 9
    1106 : Process Thread ID: 10

    1315 : Process Thread ID: 13

    1789 : MakeRequest 7 end, Thread ID: 12

```

代码清单 16

这里揭示的是:

*   这些请求似乎是分四批处理的(我正在测试的计算机有四个内核)。
*   线程正在被重用。
*   延续不会发生在同一个线程上。我们希望如此，因为这是一个控制台应用程序，我们还没有定义延续上下文。
*   因为一次只有“大致”四个线程处于活动状态，所以整个过程大约需要 2.3 秒才能完成(10 个请求/ 4 个线程的奇数是 2.5)。

相反，观察 8 核系统会发生什么:

```cs
    38 : Process Thread ID: 15
    38 : Process Thread ID: 13
    38 : Process Thread ID: 5
    38 : Process Thread ID: 16
    39 : Process Thread ID: 17
    39 : Process Thread ID: 14
    40 : Process Thread ID: 19
    41 : Process Thread ID: 18

    782 : Process Thread ID: 20
    1039 : Process Thread ID: 15

```

代码清单 17

现在，我们看到八个请求同时被处理，最后两个请求稍后发生。这是怎么回事？

### 为什么异步/等待不是正确的解决方案

从前面的跟踪中，我们可以推测为延续分配的线程是根据 CPU 内核的数量来分配的。这真的不是我们想要的行为。许多请求将涉及文件输入/输出、与数据库交互、联系社交媒体等，所有这些都是线程在等待响应时会被阻止的过程。我们当然不希望仅仅因为分配延续线程的机制认为它应该基于可用的内核而延迟其他传入请求的处理。不幸的是，这个机制似乎是如何处理延续的核心。它无法通过`TaskCreationOptions`控制，因为我们正在处理等待中的呼叫的继续是如何处理的。我们在这里只能声明，这不是我们想要的实现。

## 分配我们自己的线程

本节介绍的源代码位于[比特桶存储库](https://bitbucket.org/syncfusiontech/web-servers-succinctly)的**示例\第 3 章\演示-线程**文件夹中。

当我们自己分配线程时会发生什么？让我们试一试。首先，我们改变上下文监听器线程的初始化方式，用创建 20 个监听器线程来替换`TaskRun`和信号量:

```cs
    for (int i = 0; i < 20; i++)
    {
      Thread thread = new Thread(new ParameterizedThreadStart(WaitForConnection));
      thread.IsBackground = true;
      thread.Start(listener);
    }

```

代码清单 18

然后，代替使用`async/await`和信号量，每个线程阻塞直到接收到连接:

```cs
    /// <summary>
    /// Block until a connection is received.
    /// </summary>
    static void WaitForConnection(object objListener)
    {
      HttpListener listener = (HttpListener)objListener;

      while (true)
      {
        TimeStamp("StartConnectionListener Thread ID: " + Thread.CurrentThread.ManagedThreadId);
        HttpListenerContext context = listener.GetContext();
        handler.Process(context);
      }
    }

```

代码清单 19

现在，当我们发出请求时，我们立即看到它们由 10 个唯一的线程处理:

```cs
    75 : Process Thread ID: 3
     75 : Process Thread ID: 9
     75 : Process Thread ID: 4
     75 : Process Thread ID: 5
     76 : Process Thread ID: 8
     75 : Process Thread ID: 10
     76 : Process Thread ID: 7
     76 : Process Thread ID: 6
     76 : Process Thread ID: 11
     76 : Process Thread ID: 12

```

代码清单 20

我们还看到，这些响应都在同一个“一秒钟后”的时间段内:

```cs
    1083 : MakeRequest 4 end, Thread ID: 31
     1090 : MakeRequest 2 end, Thread ID: 31
     1098 : MakeRequest 3 end, Thread ID: 31
     1097 : MakeRequest 1 end, Thread ID: 28
     1104 : MakeRequest 0 end, Thread ID: 32
     1091 : MakeRequest 8 end, Thread ID: 29
     1113 : MakeRequest 6 end, Thread ID: 29
     1088 : MakeRequest 5 end, Thread ID: 30
     1119 : MakeRequest 7 end, Thread ID: 32
     1121 : MakeRequest 9 end, Thread ID: 29

```

代码清单 21

这毫不含糊地向我们表明，使用`async` / `await`不是正确的实现选择！

## 线程池呢？

本节介绍的源代码位于[位桶存储库](https://bitbucket.org/syncfusiontech/web-servers-succinctly)中的**示例\第 3 章\演示-线程池文件夹**中。但是是`async/**await**`的问题还是系统`ThreadPool`的问题？使用`ThreadPool`并不理想，因为我们正在实现长时间运行的线程，但是无论如何我们都会尝试:

```cs
    For (int i = 0; i < 20; i++)
    {
      ThreadPool.QueueUserWorkItem(WaitForConnection, listener);
    }

```

代码清单 22

看看初始化过程会发生什么:

```cs
    781 : StartConnectionListener Thread ID: 7
     1313 : StartConnectionListener Thread ID: 8
     1845 : StartConnectionListener Thread ID: 9
     2377 : StartConnectionListener Thread ID: 10
     2909 : StartConnectionListener Thread ID: 11
     3441 : StartConnectionListener Thread ID: 12
     3973 : StartConnectionListener Thread ID: 13
     4505 : StartConnectionListener Thread ID: 14
     5037 : StartConnectionListener Thread ID: 15
     5569 : StartConnectionListener Thread ID: 16
     6100 : StartConnectionListener Thread ID: 17

```

代码清单 23

我们当然会经历 MSDN 文档中关于 [`ThreadPool`](https://msdn.microsoft.com/en-us/library/0ka9477y%28v=vs.95%29.aspx) 的描述:“作为线程管理策略的一部分，线程池在创建线程之前会延迟。因此，当许多任务在短时间内排队时，在所有任务开始之前可能会有很长的延迟。”

幸运的是，一旦线程被初始化，我们看到处理同时发生:

```cs
    12121 : Process Thread ID: 4
     12123 : Process Thread ID: 5
     12125 : Process Thread ID: 6
     12125 : Process Thread ID: 3
     12127 : Process Thread ID: 7
     12127 : Process Thread ID: 10
     12127 : Process Thread ID: 11
     12128 : Process Thread ID: 9
     12128 : Process Thread ID: 12
     12128 : Process Thread ID: 8

```

代码清单 24

因此，尽管线程池可以工作，但它也不是正确的解决方案。正如 MSDN 文档所指出的，线程池在这里不是正确的解决方案，因为 1)我们在很短的时间内创建了大量线程，2)这些线程将在服务器的生命周期内永久运行。此外，线程可能会长时间阻塞等待连接请求——它们不是短暂的线程。

### 结论

现在很清楚，我们不应该使用`async` / `await`来实现异步连接请求。`Async` / `await`限制您根据内核数量处理请求，防止您(和 CPU)将请求处理分配给比您拥有的内核更多的线程。这肯定是一个问题，因为在您的请求处理程序中查询数据库或第三方社交媒体应用编程接口是很常见的，并且您的线程将在很大程度上等待响应，这不应该阻止其他请求的处理。

## 单线程监听器

本节中介绍的源代码位于[位桶存储库](https://bitbucket.org/syncfusiontech/web-servers-succinctly)中的文件夹**示例\第 3 章\演示-单线程侦听器**中。

除了已经确定我们需要使用线程而不是`Task` `async/await`机制之外，我们还应该考虑我们想要多个线程监听请求还是单个线程。对于单个线程，只有一个线程在侦听传入的请求。一旦收到请求，该请求就会被放入队列，线程会立即等待下一个请求。在一个单独的线程中，请求被去排队并进入一个工作线程。我们可以实现不同的算法来确定哪个工作线程对请求进行排队，但是在接下来的实现中，我们使用简单的循环算法。

我们将从一个助手类开始，该类允许我们为每个线程创建一个队列，并创建一个信号量来通知线程:

```cs
    /// <summary>
    /// Track the semaphore and context queue associated with a worker thread.
    /// </summary>
    public class ThreadSemaphore
    {
      public int QueueCount { get { return requests.Count; } }

      protected Semaphore sem;
      protected ConcurrentQueue<HttpListenerContext> requests;

      public ThreadSemaphore()
      {
        sem = new Semaphore(0, Int32.MaxValue);
        requests = new ConcurrentQueue<HttpListenerContext>();
      }

      /// <summary>
      /// Enqueue a request context and release the semaphore that
      /// a thread is waiting on.
      /// </summary>
      public void Enqueue(HttpListenerContext context)
      {
        requests.Enqueue(context);
        sem.Release();
      }

      /// <summary>
      /// Wait for the semaphore to be released.
      /// </summary>
      public void WaitOne()
      {
        sem.WaitOne();
      }

      /// <summary>
      /// Dequeue a request.
      /// </summary>
      public bool TryDequeue(out HttpListenerContext context)
      {
        return requests.TryDequeue(out context);
      }
    }

```

代码清单 25

请注意。NET 的并发集合类`ConcurrentQueue`，在代码清单 25 中。这些是处理并发读/写的高性能集合，减轻了我们必须编写线程安全集合的复杂性。

我们的处理程序不是立即处理请求，而是将请求排队并返回。一个单独的线程将请求解排队，并将其循环分配给一个工作线程。

```cs
    public class SingleThreadedQueueingHandler
    {
      protected ConcurrentQueue<HttpListenerContext> requests;
      protected Semaphore semQueue;
      protected List<ThreadSemaphore> threadPool;
      protected const int MAX_WORKER_THREADS = 20;

      public SingleThreadedQueueingHandler()
      {
        threadPool = new List<ThreadSemaphore>();
        requests = new ConcurrentQueue<HttpListenerContext>();
        semQueue = new Semaphore(0, Int32.MaxValue);
        StartThreads();
        MonitorQueue();
      }

      protected void MonitorQueue()
      {
        Task.Run(() =>
        {
          int threadIdx = 0;

          // Forever...
          while (true)
          {
            // Wait until we have received a context.
            semQueue.WaitOne();
            HttpListenerContext context;

            if (requests.TryDequeue(out context))
            {
              // In a round-robin manner, queue up the request on the current
              // thread index then increment the index.
              threadPool[threadIdx].Enqueue(context);
              threadIdx = (threadIdx + 1) % MAX_WORKER_THREADS;
            }
          }
        });
      }

      /// <summary>
      /// Enqueue the received context rather than processing it.
      /// </summary>
      public void Process(HttpListenerContext context)
      {
        requests.Enqueue(context);
        semQueue.Release();
      }

      /// <summary>
      /// Start our worker threads.
      /// </summary>
      protected void StartThreads()
      {
        for (int i = 0; i < MAX_WORKER_THREADS; i++)
        {
          Thread thread = new Thread(new ParameterizedThreadStart(ProcessThread));
          thread.IsBackground = true;
          ThreadSemaphore ts = new ThreadSemaphore();
          threadPool.Add(ts);
          thread.Start(ts);
        }
      }

      /// <summary>
      /// As a thread, we wait until there's something to do.
      /// </summary>
      protected void ProcessThread(object state)
      {
        ThreadSemaphore ts = (ThreadSemaphore)state;

        while (true)
        {
          ts.WaitOne();
          HttpListenerContext context;

          if (ts.TryDequeue(out context))
          {
            Program.TimeStamp("Processing on thread " + Thread.CurrentThread.ManagedThreadId);
            CommonResponse(context);
          }
        }
      }
    }

```

代码清单 26

结果正如我们所料——我们的 10 个请求同时开始处理，同时完成处理。

```cs
    76 : Processing on thread 4
     76 : Processing on thread 3
     76 : Processing on thread 5
     77 : Processing on thread 6
     78 : Processing on thread 7
     78 : Processing on thread 8
     79 : Processing on thread 10
     79 : Processing on thread 11
     79 : Processing on thread 9
     81 : Processing on thread 12

     1086 : MakeRequest 0 end, Thread ID: 31
     1086 : MakeRequest 8 end, Thread ID: 29
     1093 : MakeRequest 1 end, Thread ID: 29
     1094 : MakeRequest 2 end, Thread ID: 29
     1102 : MakeRequest 7 end, Thread ID: 29
     1102 : MakeRequest 9 end, Thread ID: 31
     1109 : MakeRequest 3 end, Thread ID: 31
     1110 : MakeRequest 4 end, Thread ID: 29
     1111 : MakeRequest 6 end, Thread ID: 31
     1113 : MakeRequest 5 end, Thread ID: 31

```

代码清单 27

### 结论

单线程连接排队方法的优势在于，它可以非常快速地消耗数千个请求，然后这些请求可以排队到有限数量的工作线程上。当所有工作线程都变得繁忙时，多侦听器方法将停止接受请求。在任一实现中，客户端最终都会等待其请求得到服务。第二种方法的主要优点是，您不会创建成千上万的线程来处理大量的周期。事实上，单线程侦听器方法甚至可以实现为随着卷的增加动态地开始分配更多的线程，或者甚至假脱机处理更多的服务器。这种方法是一种更加灵活的解决方案。